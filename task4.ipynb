{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task IV: Quantum Generative Adversarial Network (QGAN)\n",
    "You will explore how best to apply a quantum generative adversarial network (QGAN) to solve a High Energy Data analysis issue, more specifically, separating the signal events from the background events. You should use the Google Cirq and Tensorflow Quantum (TFQ) libraries for this task. \n",
    "A set of input samples (simulated with Delphes) is provided in NumPy NPZ format [Download Input]. In the input file, there are only 100 samples for training and 100 samples for testing so it won’t take much computing resources to accomplish this \n",
    "task. The signal events are labeled with 1 while the background events are labeled with 0. \n",
    "Be sure to show that you understand how to fine tune your machine learning model to improve the performance. The performance can be evaluated with classification accuracy or Area Under ROC Curve (AUC). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.7.1\n",
      "    latest version: 25.1.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - tensorflow-quantum=0.3.0*\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pennylane in ./.conda/lib/python3.9/site-packages (0.38.0)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in ./.conda/lib/python3.9/site-packages (3.9.4)\n",
      "Collecting tf_keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: numpy<2.0 in ./.conda/lib/python3.9/site-packages (from pennylane) (1.26.4)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.9/site-packages (from pennylane) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.9/site-packages (from pennylane) (3.2.1)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in ./.conda/lib/python3.9/site-packages (from pennylane) (0.16.0)\n",
      "Requirement already satisfied: autograd in ./.conda/lib/python3.9/site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: toml in ./.conda/lib/python3.9/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in ./.conda/lib/python3.9/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in ./.conda/lib/python3.9/site-packages (from pennylane) (0.7.1)\n",
      "Requirement already satisfied: cachetools in ./.conda/lib/python3.9/site-packages (from pennylane) (5.5.2)\n",
      "Requirement already satisfied: pennylane-lightning>=0.38 in ./.conda/lib/python3.9/site-packages (from pennylane) (0.38.0)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.9/site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.9/site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.9/site-packages (from pennylane) (24.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/lib/python3.9/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.conda/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.conda/lib/python3.9/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.conda/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf_keras)\n",
      "  Using cached tensorflow-2.19.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.21.12)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (75.8.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.conda/lib/python3.9/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.47.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached h5py-3.13.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached ml_dtypes-0.5.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.9/site-packages (from requests->pennylane) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.9/site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.9/site-packages (from requests->pennylane) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.9/site-packages (from requests->pennylane) (2025.1.31)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached optree-0.14.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (49 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.conda/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.conda/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.19.0-cp39-cp39-macosx_12_0_arm64.whl (252.5 MB)\n",
      "Using cached h5py-3.13.0-cp39-cp39-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp39-cp39-macosx_10_9_universal2.whl (667 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-macosx_12_0_arm64.whl (3.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.1-cp39-cp39-macosx_11_0_arm64.whl (327 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, ml-dtypes, mdurl, h5py, grpcio, markdown-it-py, tensorboard, rich, keras, tensorflow, tf_keras\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.47.1\n",
      "    Uninstalling grpcio-1.47.1:\n",
      "      Successfully uninstalled grpcio-1.47.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.10.1\n",
      "    Uninstalling tensorboard-2.10.1:\n",
      "      Successfully uninstalled tensorboard-2.10.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.10.0\n",
      "    Uninstalling tensorflow-2.10.0:\n",
      "      Successfully uninstalled tensorflow-2.10.0\n",
      "Successfully installed grpcio-1.71.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.1 rich-13.9.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 tf_keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge tensorflow=2.10.0\n",
    "%conda install -c conda-forge tensorflow-quantum=0.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in file: ['training_input', 'test_input']\n",
      "training_input: <class 'numpy.ndarray'> Shape: (), Dtype: object\n",
      "First few values: [{'0': array([[-0.43079088,  0.86834819, -0.92614721, -0.92662029, -0.56900862],\n",
      "        [ 0.33924198,  0.56155499,  0.93097459, -0.91631726, -0.54463516],\n",
      "        [-0.42888879,  0.87064961, -0.92782179, -0.77533991, -0.58329176],\n",
      "        [-0.43262871,  0.86128919, -0.92240878, -0.88048862, -0.49963115],\n",
      "        [-0.99925345, -0.99949586,  0.07753685, -0.84218034, -0.5149399 ],\n",
      "        [-0.99631106, -0.99775978,  0.0756427 , -0.54117216, -0.66299335],\n",
      "        [-0.42645921,  0.87141204, -0.92908723, -0.52650143, -0.62187526],\n",
      "        [ 0.34317906,  0.57125045,  0.92638556, -0.85113425, -0.40170562],\n",
      "        [-0.99904849, -0.99933931,  0.07737929, -0.81161066, -0.53550246],\n",
      "        [ 0.3371327 ,  0.55874622,  0.92996976, -0.9117092 , -0.50996097],\n",
      "        [ 0.89649306, -0.95523176, -0.66298651, -0.71276678, -0.62698893],\n",
      "        [ 0.34293232,  0.56408047,  0.93448436, -0.88789589, -0.56154273],\n",
      "        [-0.43055876,  0.86615566, -0.92532229, -0.82531102, -0.61433506],\n",
      "        [ 0.33970589,  0.56676702,  0.92567667, -0.91562035, -0.5946945 ],\n",
      "        [-0.99924224, -0.99951208,  0.07752116, -0.8360764 , -0.56981171],\n",
      "        [-0.43099755,  0.86651251, -0.925269  , -0.86698757, -0.5334677 ],\n",
      "        [-0.99937446, -0.99960218,  0.07759084, -0.84990046, -0.57999577],\n",
      "        [-0.99889821, -0.99925173,  0.07726642, -0.78825187, -0.58779546],\n",
      "        [ 0.34950661,  0.58567909,  0.91615208, -0.55392065, -0.71591931],\n",
      "        [-0.9996095 , -0.99972522,  0.07776863, -0.87858433, -0.51991104],\n",
      "        [-0.99941236, -0.99961243,  0.07764686, -0.85816986, -0.53408948],\n",
      "        [-0.99900111, -0.99932936,  0.07736239, -0.80987373, -0.54108498],\n",
      "        [-0.99903613, -0.99944072,  0.0774002 , -0.82528366, -0.58735909],\n",
      "        [-0.99865334, -0.99912034,  0.07716685, -0.77627523, -0.53754642],\n",
      "        [-0.42986358,  0.86654897, -0.925718  , -0.73862815, -0.58674809],\n",
      "        [-0.42826609,  0.8738673 , -0.929425  , -0.83386636, -0.57230481],\n",
      "        [-0.4292864 ,  0.86711   , -0.92616079, -0.67992738, -0.58893727],\n",
      "        [-0.99891246, -0.99935218,  0.07730719, -0.81056136, -0.58817068],\n",
      "        [-0.99943724, -0.99960331,  0.07761935, -0.84897875, -0.57251576],\n",
      "        [-0.43294759,  0.86021519, -0.92189498, -0.87279564, -0.59891923],\n",
      "        [-0.99916459, -0.99946953,  0.07745698, -0.82671955, -0.58793255],\n",
      "        [-0.99341325, -0.99601417,  0.07387947, -0.24695737, -0.73035246],\n",
      "        [-0.99991548, -0.99993058,  0.07794216, -0.90504651, -0.56418312],\n",
      "        [-0.99922291, -0.99953394,  0.07750004, -0.83864938, -0.5907458 ],\n",
      "        [-0.43169009,  0.8646533 , -0.92421266, -0.89228919, -0.52078182],\n",
      "        [-0.99880664, -0.99928854,  0.07727578, -0.80952183, -0.5345482 ],\n",
      "        [-0.42616344,  0.88033088, -0.93295403, -0.82686517, -0.55077716],\n",
      "        [-0.43139955,  0.86308588, -0.92365803, -0.79283965, -0.56396112],\n",
      "        [-0.42911949,  0.86960978, -0.92732509, -0.76203901, -0.59011644],\n",
      "        [-0.43195732,  0.86474019, -0.92418549, -0.92918364, -0.59731155],\n",
      "        [ 0.88467977, -0.95414347, -0.66293153, -0.82036316, -0.59800758],\n",
      "        [ 0.88789478, -0.95460466, -0.66579754, -0.74234352, -0.64920965],\n",
      "        [-0.99902527, -0.99935633,  0.07734482, -0.80286049, -0.61041081],\n",
      "        [ 0.91931151, -0.94505739, -0.67409081, -0.63394305, -0.62119434],\n",
      "        [-0.43082934,  0.86509194, -0.92477223, -0.78937098, -0.60215747],\n",
      "        [ 0.34116539,  0.5652242 ,  0.93031814, -0.91565387, -0.59557556],\n",
      "        [-0.42622734,  0.88083524, -0.93317473, -0.86019392, -0.57646517],\n",
      "        [-0.99827896, -0.99902924,  0.07692876, -0.75539493, -0.62698962],\n",
      "        [-0.99820057, -0.99867978,  0.07680484, -0.70805718, -0.47476004],\n",
      "        [-0.43178015,  0.86266855, -0.92335757, -0.83674549, -0.5820067 ]]), '1': array([[-0.42298067,  0.88630865, -0.93661218, -0.64944313, -0.39193538],\n",
      "        [ 0.90999432, -0.94429141, -0.6746157 , -0.80518637, -0.53296538],\n",
      "        [-0.99909734, -0.99933762,  0.07749262, -0.83351171, -0.393053  ],\n",
      "        [ 0.35152705,  0.5794319 ,  0.91806358, -0.00923369, -0.75412351],\n",
      "        [ 0.34399902,  0.57339474,  0.92616223, -0.91269157, -0.51149302],\n",
      "        [-0.42905645,  0.86662382, -0.92592506, -0.65770698, -0.38264457],\n",
      "        [ 0.88005236, -0.9599969 , -0.66609191, -0.8764421 , -0.58517572],\n",
      "        [-0.99793345, -0.99872791,  0.07670388, -0.71240287, -0.56441582],\n",
      "        [ 0.91213483, -0.91733005, -0.65528741, -0.67448585, -0.51199249],\n",
      "        [ 0.88551226, -0.94868142, -0.66252618, -0.90843762, -0.59915173],\n",
      "        [ 0.89150504, -0.94960047, -0.67068894,  0.15565222, -0.84421124],\n",
      "        [ 0.88909834, -0.94318762, -0.67148287, -0.84064276, -0.60912642],\n",
      "        [ 0.90255888, -0.92328057, -0.67519978, -0.57106936, -0.66341163],\n",
      "        [ 0.8976049 , -0.94774689, -0.66648593, -0.65464116, -0.52345396],\n",
      "        [-0.43030625,  0.86671232, -0.92539398, -0.82285501, -0.16938372],\n",
      "        [-0.42585346,  0.86764478, -0.9275629 , -0.32038269, -0.50555058],\n",
      "        [-0.43160184,  0.86429225, -0.92408762, -0.85714772, -0.54078138],\n",
      "        [ 0.34190452,  0.57019409,  0.9243911 , -0.8128156 , -0.58818964],\n",
      "        [ 0.89132992, -0.9483933 , -0.65619896, -0.88958426, -0.54497086],\n",
      "        [ 0.890744  , -0.95340342, -0.66924037, -0.79894801, -0.56738322],\n",
      "        [ 0.91361551, -0.94972251, -0.67585399, -0.50145697, -0.1911723 ],\n",
      "        [ 0.88935183, -0.94313315, -0.66904492, -0.79169152, -0.54132759],\n",
      "        [ 0.34476283,  0.57803763,  0.92061164, -0.80268017, -0.32524356],\n",
      "        [ 0.88926914, -0.95421273, -0.66842321, -0.68806207, -0.60578623],\n",
      "        [ 0.89505106, -0.9451298 , -0.67363669, -0.69723999, -0.66076491],\n",
      "        [ 0.362334  ,  0.59572753,  0.93213198, -0.72362975, -0.59988639],\n",
      "        [-0.4288074 ,  0.86967951, -0.92739944, -0.74532187, -0.46450815],\n",
      "        [ 0.88718043, -0.95091359, -0.65983102, -0.8940726 , -0.53211548],\n",
      "        [-0.99846294, -0.99900932,  0.07703519, -0.7582664 , -0.54351981],\n",
      "        [ 0.34178597,  0.56585585,  0.92943783, -0.85569345, -0.56113033],\n",
      "        [ 0.88296536, -0.95481506, -0.66270552, -0.78421127, -0.61179041],\n",
      "        [ 0.88426395, -0.95297205, -0.6626738 , -0.65294941, -0.46763447],\n",
      "        [-0.99590417, -0.99728851,  0.07527463, -0.45478035, -0.64937587],\n",
      "        [ 0.34028029,  0.56005082,  0.93145617, -0.76996587, -0.55614608],\n",
      "        [ 0.88938047, -0.95010936, -0.66373879, -0.80909527, -0.45277393],\n",
      "        [-0.99688379, -0.99798169,  0.07591516, -0.57297135, -0.64869519],\n",
      "        [ 0.90560361, -0.9344575 , -0.65804998, -0.7947352 , -0.58626245],\n",
      "        [ 0.3407186 ,  0.5649425 ,  0.92917793, -0.88543543, -0.45824451],\n",
      "        [ 0.34652975,  0.56915775,  0.93002691, -0.5463081 , -0.61838199],\n",
      "        [ 0.34864817,  0.58382871,  0.91784001, -0.62395809, -0.67006945],\n",
      "        [-0.4268417 ,  0.87348543, -0.92980882, -0.61323903, -0.53234345],\n",
      "        [ 0.93378275, -0.85174182, -0.70941954,  0.05491566, -0.27530413],\n",
      "        [ 0.34210266,  0.56613037,  0.92717996, -0.69534422, -0.62937982],\n",
      "        [ 0.3507178 ,  0.57835822,  0.93047705, -0.73815929, -0.53465596],\n",
      "        [ 0.35258416,  0.58849015,  0.92018786, -0.62128929, -0.58711745],\n",
      "        [ 0.92789668, -0.90635417, -0.64723127, -0.61636877, -0.534791  ],\n",
      "        [-0.43018391,  0.86569257, -0.92522163, -0.72360566, -0.50565552],\n",
      "        [ 0.3490436 ,  0.5855566 ,  0.91734464, -0.60993714, -0.58527924],\n",
      "        [ 0.34434515,  0.56629373,  0.93026591, -0.6060978 , -0.64930218],\n",
      "        [ 0.88125135, -0.95437964, -0.66664384, -0.78187561, -0.64345757]])}                                                                               ]\n",
      "test_input: <class 'numpy.ndarray'> Shape: (), Dtype: object\n",
      "First few values: [{'0': array([[-0.43080401,  0.86308617, -0.92383665, -0.7288808 , -0.53944676],\n",
      "        [ 0.33955674,  0.5637837 ,  0.92720881, -0.81843942, -0.6058888 ],\n",
      "        [ 0.88826449, -0.94407139, -0.66863101, -0.75178845, -0.60501115],\n",
      "        [-0.99698887, -0.99830749,  0.07614012, -0.64736972, -0.61107892],\n",
      "        [-0.42987561,  0.86729839, -0.92592964, -0.77358656, -0.45954928],\n",
      "        [-0.99903924, -0.99940754,  0.07740513, -0.81484426, -0.59764327],\n",
      "        [ 0.89527424, -0.93164945, -0.67830638, -0.17547695,  0.58070215],\n",
      "        [-0.99867731, -0.99920143,  0.07716389, -0.78608448, -0.58575145],\n",
      "        [-0.99954016, -0.99970837,  0.07769953, -0.86504334, -0.59019686],\n",
      "        [-0.99933588, -0.99957473,  0.07758163, -0.84371294, -0.58384261],\n",
      "        [-0.42695331,  0.87580322, -0.93070004, -0.76444273, -0.48976452],\n",
      "        [-0.99885065, -0.99927057,  0.0772765 , -0.79447449, -0.58271023],\n",
      "        [-0.42865325,  0.86878334, -0.92698692, -0.6539855 , -0.34172704],\n",
      "        [-0.99787963, -0.99863084,  0.07661562, -0.69014412, -0.58365828],\n",
      "        [-0.99837662, -0.99910865,  0.0770095 , -0.7782101 , -0.58049142],\n",
      "        [ 0.34392947,  0.5693435 ,  0.92815797, -0.74497354, -0.60182769],\n",
      "        [-0.99953497, -0.99967929,  0.0777224 , -0.86971132, -0.5269121 ],\n",
      "        [-0.99126797, -0.9947514 ,  0.07262369, -0.04116964, -0.74510938],\n",
      "        [-0.99908055, -0.99935528,  0.07737153, -0.8051729 , -0.58513343],\n",
      "        [ 0.89032902, -0.94723034, -0.66399517, -0.71871495, -0.6031206 ],\n",
      "        [-0.99943867, -0.99967164,  0.07765269, -0.86201518, -0.58284056],\n",
      "        [-0.43222666,  0.86209081, -0.92293499, -0.88012618, -0.53858141],\n",
      "        [-0.99842144, -0.99900552,  0.07700095, -0.74857524, -0.5994952 ],\n",
      "        [-0.43256898,  0.86209462, -0.92283848, -0.89380029, -0.5954924 ],\n",
      "        [-0.42912578,  0.87058639, -0.9277482 , -0.78621766, -0.59756796],\n",
      "        [-0.43157153,  0.86361893, -0.92385276, -0.84326503, -0.6164978 ],\n",
      "        [-0.43098602,  0.86253666, -0.92358306, -0.67534123, -0.50955867],\n",
      "        [-0.43094687,  0.8658162 , -0.92501124, -0.83393871, -0.58674407],\n",
      "        [-0.99866788, -0.99920531,  0.0771777 , -0.79422702, -0.54766649],\n",
      "        [-0.43273777,  0.8612637 , -0.92243091, -0.88860458, -0.60421496],\n",
      "        [-0.4322661 ,  0.86274635, -0.92320025, -0.88125417, -0.57723471],\n",
      "        [-0.99832208, -0.99889785,  0.07697653, -0.7430776 , -0.51260027],\n",
      "        [ 0.88906455, -0.94992604, -0.66296404, -0.82195036, -0.6161204 ],\n",
      "        [-0.43242199,  0.86331832, -0.92341113, -0.92948154, -0.58986346],\n",
      "        [ 0.34767309,  0.58003841,  0.92228197, -0.7514035 , -0.62374181],\n",
      "        [-0.43035101,  0.86699525, -0.9257247 , -0.81210162, -0.52763153],\n",
      "        [-0.99778784, -0.99847698,  0.07651813, -0.65872268, -0.58512892],\n",
      "        [-0.99756737, -0.99846888,  0.07650007, -0.66221651, -0.58653339],\n",
      "        [-0.43158202,  0.86098907, -0.92271089, -0.7360913 , -0.61856122],\n",
      "        [-0.99955284, -0.99970701,  0.07771111, -0.86886151, -0.56562602],\n",
      "        [-0.99942309, -0.99964124,  0.07761448, -0.85385332, -0.59691032],\n",
      "        [-0.99914884, -0.99945807,  0.07747295, -0.82914612, -0.56029987],\n",
      "        [-0.43278051,  0.86093604, -0.92225117, -0.88812466, -0.59184068],\n",
      "        [-0.42785893,  0.87013721, -0.92798689, -0.61662395, -0.55879758],\n",
      "        [ 0.34030259,  0.56241848,  0.93167332, -0.90680456, -0.55682875],\n",
      "        [-0.42977085,  0.86736676, -0.92610302, -0.78123001, -0.55259086],\n",
      "        [-0.42849968,  0.87274052, -0.92884721, -0.81527453, -0.5607667 ],\n",
      "        [-0.99818787, -0.99876499,  0.07685375, -0.72337807, -0.49215628],\n",
      "        [-0.99877827, -0.99922868,  0.07724171, -0.78754225, -0.58240214],\n",
      "        [ 0.89285346, -0.93893511, -0.66813116, -0.76865312, -0.66066306]]), '1': array([[ 0.90689447, -0.93479411, -0.65927121, -0.74293159, -0.54760953],\n",
      "        [-0.42184782,  0.89052551, -0.9389674 , -0.70861897, -0.6311667 ],\n",
      "        [ 0.34335972,  0.56596082,  0.92992177, -0.72428136, -0.13970648],\n",
      "        [-0.43004913,  0.8661446 , -0.92550141, -0.70879482, -0.58520283],\n",
      "        [ 0.35036104,  0.57228906,  0.92652707, -0.1880249 , -0.7190437 ],\n",
      "        [-0.42629431,  0.87984351, -0.93268813, -0.81858153, -0.50720301],\n",
      "        [ 0.91672213, -0.92521531, -0.67657942, -0.71835835, -0.60214219],\n",
      "        [ 0.34638486,  0.58103808,  0.91996659, -0.78488732, -0.53485255],\n",
      "        [ 0.3599873 ,  0.58959813,  0.93406185, -0.67490126, -0.59711879],\n",
      "        [ 0.88494423, -0.95662594, -0.66253887, -0.62306573, -0.64104253],\n",
      "        [-0.43116355,  0.86546918, -0.92473105, -0.86154509, -0.50922589],\n",
      "        [-0.42715205,  0.87458862, -0.93019951, -0.72716844, -0.61863442],\n",
      "        [-0.43042302,  0.86533463, -0.92495231, -0.76032081, -0.5120372 ],\n",
      "        [ 0.34697277,  0.58285707,  0.91899981, -0.79689494, -0.40087697],\n",
      "        [ 0.34629836,  0.57567028,  0.92696927, -0.85708201, -0.61931513],\n",
      "        [-0.99801587, -0.99870818,  0.07678466, -0.71725982, -0.48870255],\n",
      "        [-0.42800187,  0.87384419, -0.92947851, -0.80223706, -0.52114468],\n",
      "        [ 0.34691394,  0.57223874,  0.93203858, -0.8556948 , -0.60034294],\n",
      "        [ 0.34547356,  0.57422   ,  0.92633171, -0.81533409, -0.55736157],\n",
      "        [-0.43205391,  0.8612029 , -0.92260418, -0.8246819 , -0.52203251],\n",
      "        [ 0.93340672, -0.95165763, -0.68880997, -0.62519918, -0.55507405],\n",
      "        [ 0.90959403, -0.94089412, -0.67169727, -0.75151705, -0.6063621 ],\n",
      "        [ 0.88910571, -0.94833312, -0.66411822, -0.639409  , -0.21044972],\n",
      "        [ 0.88989098, -0.94811778, -0.65850965, -0.82851997, -0.57555645],\n",
      "        [-0.42901012,  0.87011149, -0.92758595, -0.77032681, -0.5953954 ],\n",
      "        [ 0.34244191,  0.56843782,  0.92728552, -0.80896876, -0.4551285 ],\n",
      "        [ 0.34917913,  0.57548004,  0.91886101, -0.04618507, -0.34192831],\n",
      "        [-0.43003742,  0.86602938, -0.92549205, -0.70228174, -0.63249113],\n",
      "        [-0.99870301, -0.9991251 ,  0.07715473, -0.77213351, -0.56120182],\n",
      "        [-0.99795805, -0.99856388,  0.07686491, -0.73313579, -0.22159224],\n",
      "        [ 0.34834713,  0.57235101,  0.9277259 , -0.45359207, -0.48160926],\n",
      "        [-0.42184782,  0.89052551, -0.9389674 , -0.70861897, -0.6311667 ],\n",
      "        [ 0.88990737, -0.95061773, -0.66311951, -0.82060348, -0.58798499],\n",
      "        [-0.43014458,  0.86975118, -0.92696748, -0.90428701, -0.55016704],\n",
      "        [-0.9985547 , -0.99905237,  0.07713925, -0.77271806, -0.48992786],\n",
      "        [-0.99858757, -0.99908176,  0.07706081, -0.76060934, -0.60066967],\n",
      "        [ 0.90188487, -0.94156087, -0.661881  , -0.21453967, -0.64135111],\n",
      "        [ 0.88156777, -0.95896986, -0.66323095, -0.81024967, -0.63449728],\n",
      "        [ 0.90276812, -0.91913964, -0.67116558, -0.68397294, -0.60849373],\n",
      "        [ 0.89724099, -0.95416238, -0.66312957, -0.84026921, -0.62179175],\n",
      "        [ 0.34311354,  0.56575923,  0.92888695, -0.68097658, -0.66348147],\n",
      "        [ 0.88484065, -0.95250068, -0.66693508, -0.74875858, -0.50433469],\n",
      "        [ 0.3400326 ,  0.5650874 ,  0.92595663, -0.77808114, -0.42764128],\n",
      "        [-0.41910386,  0.8840282 , -0.93692662, -0.04135383, -0.35382193],\n",
      "        [-0.42456182,  0.87854429, -0.93276891, -0.60893304, -0.48509134],\n",
      "        [-0.42984943,  0.86623583, -0.92559676, -0.75127326, -0.55676617],\n",
      "        [ 0.34489673,  0.56830642,  0.93350345, -0.89592489, -0.62520496],\n",
      "        [ 0.89630181, -0.93506415, -0.67309517, -0.85184444, -0.57296623],\n",
      "        [ 0.88812193, -0.95006594, -0.66474068, -0.74195712, -0.59825518],\n",
      "        [ 0.34815659,  0.57541096,  0.92986494, -0.8133517 , -0.61248815]])}                                                                               ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the file with pickle support\n",
    "data = np.load('./QIS_EXAM_200Events.npz', allow_pickle=True)\n",
    "\n",
    "# Print keys\n",
    "print(\"Keys in file:\", list(data.keys()))\n",
    "\n",
    "# Examine each key\n",
    "for key in data.keys():\n",
    "    arr = data[key]\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        print(f\"{key}: {type(arr)} Shape: {arr.shape}, Dtype: {arr.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(arr)}\")\n",
    "        \n",
    "    # Try to print first few values\n",
    "    try:\n",
    "        if isinstance(arr, np.ndarray) and arr.size > 0:\n",
    "            print(\"First few values:\", arr.flatten()[:5])\n",
    "        else:\n",
    "            print(\"Value:\", arr)\n",
    "    except:\n",
    "        print(\"Cannot display values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data loading and processing\n",
    "def load_and_process_data(file_path):\n",
    "    \"\"\"\n",
    "    Simplified extraction based on the observed structure\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Extract the training features from the dictionary\n",
    "    training_dict = data['training_input'].item()  # Get the dictionary from the array\n",
    "    training_key = list(training_dict.keys())[0]  # Get the first key ('0')\n",
    "    X_train = training_dict[training_key]  # Get the feature array\n",
    "    \n",
    "    # Extract the test features from the dictionary\n",
    "    test_dict = data['test_input'].item()  # Get the dictionary from the array\n",
    "    test_key = list(test_dict.keys())[0]  # Get the first key ('0')\n",
    "    X_test = test_dict[test_key]  # Get the feature array\n",
    "    \n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Test features shape: {X_test.shape}\")\n",
    "    \n",
    "    # Create labels (assuming we need to create binary labels)\n",
    "    # Method 1: Using principal component\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    train_pca = pca.fit_transform(X_train)\n",
    "    test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Use median as threshold for binary labels\n",
    "    train_threshold = np.median(train_pca)\n",
    "    y_train = (train_pca > train_threshold).astype(int).ravel()\n",
    "    y_test = (test_pca > train_threshold).astype(int).ravel()\n",
    "    \n",
    "    print(f\"Created labels - Training: {np.sum(y_train)} signal events, {len(y_train) - np.sum(y_train)} background events\")\n",
    "    print(f\"Created labels - Test: {np.sum(y_test)} signal events, {len(y_test) - np.sum(y_test)} background events\")\n",
    "    \n",
    "    # Scale the features to [0, π] for quantum circuit\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "# Dimension reduction if needed\n",
    "def apply_pca(X_train, X_test, n_components=None):\n",
    "    \"\"\"Apply PCA dimension reduction if the feature dimension is too large\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Determine optimal number of components to retain 95% variance\n",
    "    if n_components is None:\n",
    "        pca = PCA(n_components=0.95)\n",
    "        pca.fit(X_train)\n",
    "        n_components = pca.n_components_\n",
    "    else:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca.fit(X_train)\n",
    "    \n",
    "    print(f\"Reducing dimensions from {X_train.shape[1]} to {n_components} with PCA\")\n",
    "    print(f\"Explained variance ratio: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    return X_train_pca, X_test_pca, pca\n",
    "\n",
    "# Define quantum devices\n",
    "def setup_quantum_circuit(n_qubits):\n",
    "    \"\"\"Setup quantum circuit devices for generator and discriminator\"\"\"\n",
    "    print(f\"Setting up quantum circuits with {n_qubits} qubits\")\n",
    "    \n",
    "    # Create PennyLane device for generator\n",
    "    dev_gen = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    \n",
    "    # Create PennyLane device for discriminator\n",
    "    dev_disc = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    \n",
    "    # Define the quantum circuit for the generator\n",
    "    @qml.qnode(dev_gen)\n",
    "    def quantum_generator(noise, weights):\n",
    "        \"\"\"Variational quantum circuit for generator\"\"\"\n",
    "        # Initialize with Hadamard gates\n",
    "        for i in range(n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "        \n",
    "        # Encode input noise\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(noise[i], wires=i)\n",
    "        \n",
    "        # Variational layers\n",
    "        n_layers = len(weights) // (2 * n_qubits)\n",
    "        for l in range(n_layers):\n",
    "            # Rotation layer\n",
    "            for i in range(n_qubits):\n",
    "                qml.RX(weights[l*2*n_qubits + i], wires=i)\n",
    "                qml.RZ(weights[l*2*n_qubits + n_qubits + i], wires=i)\n",
    "            \n",
    "            # Entanglement layer\n",
    "            for i in range(n_qubits-1):\n",
    "                qml.CNOT(wires=[i, i+1])\n",
    "        \n",
    "        # Return expectation values\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "    \n",
    "    # Define the quantum circuit for the discriminator\n",
    "    @qml.qnode(dev_disc)\n",
    "    def quantum_discriminator(inputs, weights):\n",
    "        \"\"\"Quantum circuit for feature extraction in the discriminator\"\"\"\n",
    "        # Encode input data\n",
    "        for i in range(n_qubits):\n",
    "            qml.RX(inputs[i], wires=i)\n",
    "        \n",
    "        # Entanglement layer\n",
    "        for i in range(n_qubits-1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "        \n",
    "        # Rotation layer with trainable weights\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(weights[i], wires=i)\n",
    "        \n",
    "        # Return expectation values\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "    \n",
    "    return quantum_generator, quantum_discriminator\n",
    "\n",
    "# Build the generator model\n",
    "class QuantumGenerator(tf.keras.Model):\n",
    "    def __init__(self, n_qubits, n_layers=2):\n",
    "        super(QuantumGenerator, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Set up quantum circuit\n",
    "        self.quantum_generator, _ = setup_quantum_circuit(n_qubits)\n",
    "        \n",
    "        # Classical pre-processing\n",
    "        self.dense = tf.keras.layers.Dense(n_qubits, activation='tanh')\n",
    "        \n",
    "        # Initialize weights for quantum circuit\n",
    "        weight_shapes = {\"weights\": (2 * n_qubits * n_layers,)}\n",
    "        self.qlayer = qml.qnn.KerasLayer(self.quantum_generator, weight_shapes, output_dim=n_qubits)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return self.qlayer(x)\n",
    "\n",
    "# Build the discriminator model\n",
    "class QuantumDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, n_qubits):\n",
    "        super(QuantumDiscriminator, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        \n",
    "        # Set up quantum circuit\n",
    "        _, self.quantum_discriminator = setup_quantum_circuit(n_qubits)\n",
    "        \n",
    "        # Initialize weights for quantum circuit\n",
    "        weight_shapes = {\"weights\": (n_qubits,)}\n",
    "        self.qlayer = qml.qnn.KerasLayer(self.quantum_discriminator, weight_shapes, output_dim=n_qubits)\n",
    "        \n",
    "        # Classical post-processing\n",
    "        self.dense1 = tf.keras.layers.Dense(8, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.qlayer(inputs)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Build the GAN\n",
    "class QGAN(tf.keras.Model):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(QGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        generated = self.generator(inputs)\n",
    "        return self.discriminator(generated)\n",
    "\n",
    "# Train the GAN\n",
    "def train_qgan(generator, discriminator, gan, X_train, y_train, epochs=50, batch_size=16):\n",
    "    # Define optimizers\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Define loss function\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    \n",
    "    # Prepare real signal data\n",
    "    signal_data = X_train[y_train == 1]\n",
    "    background_data = X_train[y_train == 0]\n",
    "    \n",
    "    print(f\"Training with {signal_data.shape[0]} signal events and {background_data.shape[0]} background events\")\n",
    "    \n",
    "    history = {'disc_loss': [], 'gen_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # Sample real signal data\n",
    "            real_indices = np.random.randint(0, signal_data.shape[0], batch_size//2)\n",
    "            real_signal = signal_data[real_indices]\n",
    "            real_labels = np.ones((batch_size//2, 1))\n",
    "            \n",
    "            # Sample real background data\n",
    "            bg_indices = np.random.randint(0, background_data.shape[0], batch_size//2)\n",
    "            real_bg = background_data[bg_indices]\n",
    "            bg_labels = np.zeros((batch_size//2, 1))\n",
    "            \n",
    "            # Combine real datasets\n",
    "            real_samples = np.vstack([real_signal, real_bg])\n",
    "            real_labels = np.vstack([real_labels, bg_labels])\n",
    "            \n",
    "            # Generate fake signal data\n",
    "            noise = np.random.normal(0, 1, (batch_size, generator.n_qubits))\n",
    "            fake_samples = generator(noise)\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "            \n",
    "            # Forward pass\n",
    "            real_predictions = discriminator(real_samples)\n",
    "            fake_predictions = discriminator(fake_samples)\n",
    "            \n",
    "            # Calculate loss\n",
    "            real_loss = loss_fn(real_labels, real_predictions)\n",
    "            fake_loss = loss_fn(fake_labels, fake_predictions)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "        \n",
    "        # Update discriminator\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "        \n",
    "        # Train generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # Generate fake samples\n",
    "            noise = np.random.normal(0, 1, (batch_size, generator.n_qubits))\n",
    "            fake_samples = generator(noise)\n",
    "            \n",
    "            # Get discriminator predictions\n",
    "            predictions = discriminator(fake_samples)\n",
    "            \n",
    "            # Use real labels for generator loss (trying to fool discriminator)\n",
    "            gen_loss = loss_fn(np.ones((batch_size, 1)), predictions)\n",
    "        \n",
    "        # Update generator\n",
    "        gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "        \n",
    "        history['disc_loss'].append(float(disc_loss))\n",
    "        history['gen_loss'].append(float(gen_loss))\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Discriminator Loss: {float(disc_loss):.4f}, Generator Loss: {float(gen_loss):.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['disc_loss'], label='Discriminator Loss')\n",
    "    plt.plot(history['gen_loss'], label='Generator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('QGAN Training History')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(discriminator, X_test, y_test):\n",
    "    \"\"\"Evaluate the model on test data\"\"\"\n",
    "    y_pred = discriminator.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Signal'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, roc_auc\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def hyperparameter_tuning(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Simple hyperparameter tuning\"\"\"\n",
    "    best_auc = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    # Define hyperparameter space\n",
    "    n_layers_options = [1, 2]\n",
    "    learning_rates = [0.0001, 0.001]\n",
    "    batch_sizes = [8, 16]\n",
    "    \n",
    "    for n_layers in n_layers_options:\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(f\"Testing: n_layers={n_layers}, lr={lr}, batch_size={batch_size}\")\n",
    "                \n",
    "                # Initialize models\n",
    "                generator = QuantumGenerator(n_qubits=X_train.shape[1], n_layers=n_layers)\n",
    "                discriminator = QuantumDiscriminator(n_qubits=X_train.shape[1])\n",
    "                gan = QGAN(generator, discriminator)\n",
    "                \n",
    "                # Compile models\n",
    "                discriminator.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='binary_crosssentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "                \n",
    "                gan.compile(\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='binary_crosssentropy'\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                history = train_qgan(\n",
    "                    generator, discriminator, gan,\n",
    "                    X_train, y_train,\n",
    "                    epochs=15,  # Reduced for hyperparameter search\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                accuracy, auc_score = evaluate_model(discriminator, X_test, y_test)\n",
    "                \n",
    "                # Store result\n",
    "                results.append({\n",
    "                    'n_layers': n_layers,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'accuracy': accuracy,\n",
    "                    'auc': auc_score\n",
    "                })\n",
    "                \n",
    "                # Check if better\n",
    "                if auc_score > best_auc:\n",
    "                    best_auc = auc_score\n",
    "                    best_params = {\n",
    "                        'n_layers': n_layers,\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    # Plot hyperparameter results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = range(len(results))\n",
    "    plt.bar(x, [r['auc'] for r in results], alpha=0.6, label='AUC')\n",
    "    plt.xticks(x, [f\"L{r['n_layers']},LR{r['learning_rate']},B{r['batch_size']}\" for r in results], rotation=90)\n",
    "    plt.xlabel('Hyperparameters')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Hyperparameter Tuning Results')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_params, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 01:45:12.045309: E tensorflow/core/lib/monitoring/collection_registry.cc:81] Cannot register 2 metrics with the same name: /tensorflow/api/enable_tensor_equality\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import tf_keras instead of keras\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/__internal__/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/__internal__/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the TF-Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/applications/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/applications/convnext.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/backend.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m object_identity\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/engine/keras_tensor.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m object_identity\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/utils/__init__.py:53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Preprocessing utils\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_space\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureSpace\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Internal\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_source_inputs\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/utils/feature_space.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m regularizers\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lazy_variable\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m input_spec\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tf_keras/src/dtensor/lazy_variable.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_resource_variable_ops\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tensorflow/python/framework/tensor.py:49\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m object_identity\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m---> 49\u001b[0m _tensor_equality_api_usage_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/enable_tensor_equality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhether ops.enable_tensor_equality() is called.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_override_helper\u001b[39m(clazz_object, operator, func):\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Overrides (string) operator on Tensors to call func.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    ValueError: If operator is not allowed to be overwritten.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tensorflow/python/eager/monitoring.py:356\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[0;34m(self, name, description, *labels)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GSoC/.conda/lib/python3.9/site-packages/tensorflow/python/eager/monitoring.py:131\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[0;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# Set environment variable to use legacy Keras (Keras 2) with TensorFlow\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "# Import tf_keras instead of keras\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sympy\n",
    "import cirq\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "def main():\n",
    "    print(\"Loading and processing data...\")\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test, _ = load_and_process_data('./QIS_EXAM_200Events.npz')\n",
    "        \n",
    "        # Check feature dimensions - quantum circuits work better with fewer qubits\n",
    "        n_features = X_train.shape[1]\n",
    "        if n_features > 8:\n",
    "            print(f\"High feature dimension detected ({n_features}). Applying PCA...\")\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=8)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "            n_features = X_train.shape[1]\n",
    "        \n",
    "        print(f\"Building quantum models with {n_features} features/qubits\")\n",
    "        \n",
    "        # Initialize models using the function-based approach\n",
    "        generator = build_generator(n_qubits=n_features, n_layers=2)\n",
    "        discriminator = build_discriminator(n_qubits=n_features)\n",
    "        \n",
    "        # Build GAN\n",
    "        gan = build_qgan(generator, discriminator)\n",
    "        \n",
    "        # Compile models\n",
    "        discriminator.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        gan.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy'\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        print(\"Training QGAN...\")\n",
    "        history = train_qgan(generator, discriminator, gan, X_train, y_train, epochs=50, batch_size=16)\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"Evaluating model...\")\n",
    "        accuracy, auc_score = evaluate_model(discriminator, X_test, y_test)\n",
    "        \n",
    "        # Optional: Uncomment to run hyperparameter tuning\n",
    "        run_tuning = input(\"Do you want to run hyperparameter tuning? (y/n): \").lower() == 'y'\n",
    "        if run_tuning:\n",
    "            print(\"Running hyperparameter tuning...\")\n",
    "            best_params = tune_hyperparameters(X_train, y_train, X_test, y_test, n_features)\n",
    "            \n",
    "            # Train final model with best params\n",
    "            print(f\"Training final model with best parameters: {best_params}\")\n",
    "            generator = build_generator(n_qubits=n_features, n_layers=best_params['n_layers'])\n",
    "            discriminator = build_discriminator(n_qubits=n_features)\n",
    "            gan = build_qgan(generator, discriminator)\n",
    "            \n",
    "            discriminator.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=best_params['lr']),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            gan.compile(\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=best_params['lr']),\n",
    "                loss='binary_crossentropy'\n",
    "            )\n",
    "            \n",
    "            history = train_qgan(\n",
    "                generator, discriminator, gan,\n",
    "                X_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=best_params['batch_size']\n",
    "            )\n",
    "            \n",
    "            print(\"Evaluating final model...\")\n",
    "            accuracy, auc_score = evaluate_model(discriminator, X_test, y_test)\n",
    "        \n",
    "        print(\"QGAN implementation complete\")\n",
    "        return accuracy, auc_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Simplified code to extract and process data from your structure\n",
    "def extract_data_simple(file_path):\n",
    "    \"\"\"\n",
    "    Simplified extraction based on the observed structure\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Extract the training features from the dictionary\n",
    "    training_dict = data['training_input'].item()  # Get the dictionary from the array\n",
    "    training_key = list(training_dict.keys())[0]  # Get the first key ('0')\n",
    "    X_train = training_dict[training_key]  # Get the feature array\n",
    "    \n",
    "    # Extract the test features from the dictionary\n",
    "    test_dict = data['test_input'].item()  # Get the dictionary from the array\n",
    "    test_key = list(test_dict.keys())[0]  # Get the first key ('0')\n",
    "    X_test = test_dict[test_key]  # Get the feature array\n",
    "    \n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "    print(f\"Test features shape: {X_test.shape}\")\n",
    "    \n",
    "    # Create labels (assuming we need to create binary labels)\n",
    "    # Method 1: Using principal component\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    train_pca = pca.fit_transform(X_train)\n",
    "    test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Use median as threshold for binary labels\n",
    "    train_threshold = np.median(train_pca)\n",
    "    y_train = (train_pca > train_threshold).astype(int).ravel()\n",
    "    y_test = (test_pca > train_threshold).astype(int).ravel()\n",
    "    \n",
    "    print(f\"Created labels - Training: {np.sum(y_train)} signal events, {len(y_train) - np.sum(y_train)} background events\")\n",
    "    print(f\"Created labels - Test: {np.sum(y_test)} signal events, {len(y_test) - np.sum(y_test)} background events\")\n",
    "    \n",
    "    # Scale the features to [0, π] for quantum circuit\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "# Quantum Circuit Setup for TFQ\n",
    "def create_quantum_circuits(n_qubits, n_layers=2):\n",
    "    \"\"\"Create quantum circuits for the QGAN\"\"\"\n",
    "    # Create qubits\n",
    "    qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "    \n",
    "    # Create generator circuit\n",
    "    gen_circuit = cirq.Circuit()\n",
    "    # Initial Hadamard layer\n",
    "    gen_circuit.append(cirq.H.on_each(qubits))\n",
    "    \n",
    "    # Variational layers\n",
    "    for l in range(n_layers):\n",
    "        # Rotation layer\n",
    "        for i, q in enumerate(qubits):\n",
    "            gen_circuit.append(cirq.rx(sympy.Symbol(f'rx_{l}_{i}'))(q))\n",
    "            gen_circuit.append(cirq.rz(sympy.Symbol(f'rz_{l}_{i}'))(q))\n",
    "        \n",
    "        # Entanglement layer\n",
    "        for i in range(n_qubits-1):\n",
    "            gen_circuit.append(cirq.CNOT(qubits[i], qubits[i+1]))\n",
    "    \n",
    "    # Create discriminator circuit\n",
    "    disc_circuit = cirq.Circuit()\n",
    "    \n",
    "    # Data encoding layer\n",
    "    for i, q in enumerate(qubits):\n",
    "        disc_circuit.append(cirq.rx(sympy.Symbol(f'x_{i}'))(q))\n",
    "    \n",
    "    # Entanglement layer\n",
    "    for i in range(n_qubits-1):\n",
    "        disc_circuit.append(cirq.CNOT(qubits[i], qubits[i+1]))\n",
    "    \n",
    "    # Rotation layer\n",
    "    for i, q in enumerate(qubits):\n",
    "        disc_circuit.append(cirq.ry(sympy.Symbol(f'ry_{i}'))(q))\n",
    "    \n",
    "    return gen_circuit, disc_circuit, qubits\n",
    "\n",
    "# Build generator model\n",
    "def build_generator(n_qubits, n_layers=2):\n",
    "    \"\"\"Build the generator model\"\"\"\n",
    "    # Create input layer for noise\n",
    "    noise_input = keras.layers.Input(shape=(n_qubits,))\n",
    "    \n",
    "    # Dense layer to preprocess noise\n",
    "    x = keras.layers.Dense(n_qubits, activation='tanh')(noise_input)\n",
    "    \n",
    "    # Create the quantum circuit\n",
    "    gen_circuit, _, qubits = create_quantum_circuits(n_qubits, n_layers)\n",
    "    \n",
    "    # Create parameter symbols for the circuit\n",
    "    params = []\n",
    "    for l in range(n_layers):\n",
    "        for i in range(n_qubits):\n",
    "            params.append(sympy.Symbol(f'rx_{l}_{i}'))\n",
    "            params.append(sympy.Symbol(f'rz_{l}_{i}'))\n",
    "    \n",
    "    # Create a PQC layer\n",
    "    # First, convert the circuit to a TFQ object and set up readout operators\n",
    "    readout_operators = [cirq.Z(q) for q in qubits]\n",
    "    expectation_layer = tfq.layers.ControlledPQC(\n",
    "        gen_circuit, readout_operators, \n",
    "        initializer=tf.keras.initializers.RandomUniform(0, 2*np.pi)\n",
    "    )\n",
    "    \n",
    "    # Connect inputs to the quantum circuit\n",
    "    quantum_output = expectation_layer([x, tf.zeros_like(x)])\n",
    "    \n",
    "    # Return the generator model\n",
    "    return keras.Model(inputs=noise_input, outputs=quantum_output, name=\"Generator\")\n",
    "\n",
    "# Build discriminator model\n",
    "def build_discriminator(n_qubits):\n",
    "    \"\"\"Build the discriminator model\"\"\"\n",
    "    # Create input layer for features\n",
    "    feature_input = keras.layers.Input(shape=(n_qubits,))\n",
    "    \n",
    "    # Create the quantum circuit\n",
    "    _, disc_circuit, qubits = create_quantum_circuits(n_qubits)\n",
    "    \n",
    "    # Create parameter symbols for the circuit\n",
    "    params = [sympy.Symbol(f'ry_{i}') for i in range(n_qubits)]\n",
    "    \n",
    "    # Create a PQC layer\n",
    "    readout_operators = [cirq.Z(q) for q in qubits]\n",
    "    expectation_layer = tfq.layers.PQC(\n",
    "        disc_circuit, readout_operators,\n",
    "        initializer=tf.keras.initializers.RandomUniform(0, 2*np.pi)\n",
    "    )\n",
    "    \n",
    "    # Connect inputs to the quantum circuit\n",
    "    quantum_output = expectation_layer(feature_input)\n",
    "    \n",
    "    # Add classical post-processing\n",
    "    x = keras.layers.Dense(8, activation='relu')(quantum_output)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Return the discriminator model\n",
    "    return keras.Model(inputs=feature_input, outputs=output, name=\"Discriminator\")\n",
    "\n",
    "# Build the QGAN model\n",
    "def build_qgan(generator, discriminator):\n",
    "    \"\"\"Build the complete GAN model\"\"\"\n",
    "    # For the combined model, we only train the generator\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # The generator takes noise as input and generates fake samples\n",
    "    z = keras.layers.Input(shape=(generator.input_shape[1],))\n",
    "    fake_samples = generator(z)\n",
    "    \n",
    "    # The discriminator determines the validity of the fake samples\n",
    "    validity = discriminator(fake_samples)\n",
    "    \n",
    "    # The combined model (generator + discriminator)\n",
    "    return keras.Model(inputs=z, outputs=validity, name=\"QGAN\")\n",
    "\n",
    "# Train the QGAN\n",
    "def train_qgan(generator, discriminator, gan, X_train, y_train, epochs=50, batch_size=16):\n",
    "    # Prepare real signal and background data\n",
    "    signal_data = X_train[y_train == 1]\n",
    "    background_data = X_train[y_train == 0]\n",
    "    \n",
    "    # Training loop\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        # Select real samples\n",
    "        signal_idx = np.random.randint(0, signal_data.shape[0], batch_size // 2)\n",
    "        bg_idx = np.random.randint(0, background_data.shape[0], batch_size // 2)\n",
    "        \n",
    "        real_signal = signal_data[signal_idx]\n",
    "        real_bg = background_data[bg_idx]\n",
    "        \n",
    "        real_samples = np.vstack([real_signal, real_bg])\n",
    "        real_labels = np.vstack([\n",
    "            np.ones((batch_size // 2, 1)),\n",
    "            np.zeros((batch_size // 2, 1))\n",
    "        ])\n",
    "        \n",
    "        # Generate fake samples\n",
    "        noise = np.random.normal(0, 1, (batch_size, generator.input_shape[1]))\n",
    "        fake_samples = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, generator.input_shape[1]))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "        \n",
    "        # Store losses\n",
    "        d_losses.append(d_loss[0])\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss[0]:.4f} | G Loss: {g_loss:.4f}\")\n",
    "    \n",
    "    # Plot loss history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('QGAN Training Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'d_loss': d_losses, 'g_loss': g_losses}\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(discriminator, X_test, y_test):\n",
    "    # Get predictions\n",
    "    y_pred = discriminator.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, roc_auc\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def tune_hyperparameters(X_train, y_train, X_test, y_test, n_qubits):\n",
    "    results = []\n",
    "    \n",
    "    # Hyperparameters to test\n",
    "    n_layers_options = [1, 2]\n",
    "    learning_rates = [0.001, 0.01]\n",
    "    batch_sizes = [8, 16]\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for n_layers in n_layers_options:\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(f\"Testing: n_layers={n_layers}, lr={lr}, batch_size={batch_size}\")\n",
    "                \n",
    "                # Build models\n",
    "                generator = build_generator(n_qubits, n_layers)\n",
    "                discriminator = build_discriminator(n_qubits)\n",
    "                \n",
    "                # Compile models\n",
    "                discriminator.compile(\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "                \n",
    "                gan = build_qgan(generator, discriminator)\n",
    "                gan.compile(\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                    loss='binary_crossentropy'\n",
    "                )\n",
    "                \n",
    "                # Train with reduced epochs for tuning\n",
    "                train_qgan(generator, discriminator, gan, X_train, y_train, \n",
    "                          epochs=20, batch_size=batch_size)\n",
    "                \n",
    "                # Evaluate\n",
    "                _, auc_score = evaluate_model(discriminator, X_test, y_test)\n",
    "                \n",
    "                # Record results\n",
    "                results.append({\n",
    "                    'n_layers': n_layers,\n",
    "                    'lr': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'auc': auc_score\n",
    "                })\n",
    "                \n",
    "                # Update best parameters\n",
    "                if auc_score > best_auc:\n",
    "                    best_auc = auc_score\n",
    "                    best_params = {\n",
    "                        'n_layers': n_layers,\n",
    "                        'lr': lr,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You might need to install these packages:\n",
    "    # !pip install tf_keras tensorflow_quantum sympy cirq\n",
    "    \n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"\\nIf you're encountering dependency issues, try installing the required packages:\")\n",
    "        print(\"pip install tf_keras tensorflow_quantum cirq sympy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
